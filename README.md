# ğŸš€ RAG System with Groq, ChromaDB & Sentence Transformers

**A complete beginner-friendly guide with step-by-step instructions**

This project demonstrates how to build a **Retrieval-Augmented Generation (RAG) System** using:

- âš¡ **Groq LLMs** (super-fast inference)
- ğŸ—„ï¸ **ChromaDB** (local vector database)
- ğŸ§  **Sentence Transformers** (embeddings)
- ğŸ“Š **JSON Data Processing** (publication documents)
- ğŸ **Python**

This README will help you understand, set up, run, and extend your RAG system â€” **even if you are new to AI/LLMs**.

---

## ğŸ§  What You Will Build

By the end of this project, you will have:

âœ… A working LLM connection using **Groq**  
âœ… A document loader for your **JSON publications**  
âœ… A vector database (**ChromaDB**)  
âœ… A text chunking & embeddings pipeline  
âœ… A **RAG pipeline** that answers questions with citations  
âœ… Fully working code files:
- `test_llm.py`
- `json_processor.py`
- `vector_store.py`
- `rag_pipeline.py`

---

## ğŸŒŸ Key Features

- ğŸš€ **Lightning Fast**: Powered by Groq's ultra-fast LLM inference
- ğŸ” **Semantic Search**: ChromaDB vector database for intelligent document retrieval
- ğŸ“Š **JSON Data Processing**: Automatically processes publication data
- ğŸ§  **Intelligent Chunking**: Smart text chunking with overlap for better context
- ğŸ’¾ **Persistent Storage**: ChromaDB saves embeddings locally
- ğŸ¯ **Context-Aware Answers**: Retrieves most relevant documents before generating responses
- ğŸ’° **Cost-Effective**: Local embeddings (no API costs)
- ğŸ”§ **Modular Design**: Clean, reusable code structure

---

## ğŸ—ï¸ System Architecture

```
User Question
     â†“
[Vector Search] â†’ ChromaDB (finds relevant docs)
     â†“
[Context Building] â†’ Combines relevant publications
     â†“
[LLM Generation] â†’ Groq API (llama-3.1-8b-instant)
     â†“
Generated Answer
```

## ğŸ“‹ Prerequisites

- Python 3.8+
- Groq API Key (FREE at [console.groq.com](https://console.groq.com/))
- Basic understanding of Python

---

## ğŸš€ Quick Start Guide

### ğŸ”¥ Step 1: Create Groq API Key

1. Go to: https://console.groq.com
2. Sign in (create account if needed)
3. Click **API Keys** â†’ **Create Key**
4. Copy the key starting with `gsk_...`

### ğŸ›  Step 2: Set Up Project

```bash
# Create project directory
mkdir my-rag-project
cd my-rag-project

# Create virtual environment
python -m venv rag_env

# Activate virtual environment
rag_env\Scripts\activate     # Windows
# source rag_env/bin/activate  # Mac/Linux

# Install required packages
pip install groq chromadb sentence-transformers tiktoken python-dotenv
```

### ğŸ”‘ Step 3: Configure Environment Variables

Create a `.env` file in the project root:

```env
GROQ_API_KEY=gsk_your_api_key_here
```

**Get your FREE Groq API key**: https://console.groq.com/

### ğŸ¤– Step 4: Test LLM Connection

Create `test_llm.py` to verify Groq is working:

```python
import os
from dotenv import load_dotenv
from groq import Groq

load_dotenv()

client = Groq(api_key=os.getenv('GROQ_API_KEY'))

response = client.chat.completions.create(
    model="llama-3.1-8b-instant",
    messages=[{"role": "user", "content": "Hello! Can you hear me?"}]
)

print("âœ… LLM Response:", response.choices[0].message.content)
```

**Run it:**
```bash
python test_llm.py
```

**Expected Output:**
```
âœ… LLM Response: Yes, I can hear you! How can I help you today?
```

### ğŸ“˜ Step 5: Prepare Your Data

Place your JSON publication data in the `documents/` folder:

```
documents/
  â””â”€â”€ project_1_publications.json
```

**Expected JSON format:**
```json
[
  {
    "id": "1",
    "title": "Publication Title",
    "username": "author_name",
    "publication_description": "Full publication content here..."
  }
]
```

### ğŸ“¦ Step 6: Build Vector Store

Run the vector store script to process and embed your documents:

```bash
python vector_store.py
```

**Expected Output:**
```
ğŸ” Loading documents from JSON...
âœ… Loaded 45 publications
ğŸ§© Chunking documents...
âœ… Created 324 chunks
ğŸ’¾ Adding to vector store...
âœ… Vector store ready!
```

### ğŸ¯ Step 7: Run RAG System

```bash
python rag_pipeline.py
```

**Try asking:**
- "What are the main research topics?"
- "Summarize the key findings"
- "What methodologies are discussed?"

Type `quit` to exit.

## ğŸ“‚ Project Structure

```
my-rag-project/
â”œâ”€â”€ ğŸ“„ rag_pipeline.py         # Main RAG system
â”œâ”€â”€ ğŸ“„ vector_store.py          # ChromaDB vector operations
â”œâ”€â”€ ğŸ“„ json_processor.py        # JSON data loader
â”œâ”€â”€ ğŸ“„ test_llm.py             # LLM connection test
â”œâ”€â”€ ğŸ“ documents/               # Your JSON data
â”‚   â””â”€â”€ project_1_publications.json
â”œâ”€â”€ ğŸ“ chroma_db/              # Auto-generated vector DB
â”œâ”€â”€ ğŸ” .env                     # API keys (create this)
â””â”€â”€ ğŸ“ rag_env/                # Virtual environment
```

### ğŸ” Component Details

#### `json_processor.py`
Loads publication data from JSON files:
```python
def load_publications(file_path):
    # Reads JSON
    # Filters publications (min 100 chars)
    # Returns structured documents
```

**What it does:**
- Reads JSON file from `documents/` folder
- Extracts: `id`, `title`, `username`, `publication_description`
- Filters short publications (< 100 characters)
- Returns clean list of documents

#### `vector_store.py`
Manages ChromaDB vector operations:
```python
class VectorStore:
    # Creates embeddings with SentenceTransformer
    # Chunks text (512 tokens, 50 overlap)
    # Stores in ChromaDB
    # Searches by similarity
```

**What it does:**
- **Chunking**: Splits long texts into 512-token chunks with 50-token overlap
- **Embedding**: Converts text to vectors using `all-MiniLM-L6-v2`
- **Storage**: Saves vectors to persistent ChromaDB (`chroma_db/` folder)
- **Search**: Finds most similar chunks to your query

#### `rag_pipeline.py`
Main RAG system:
```python
class GroqRAGSystem:
    def query(self, question):
        # 1. Search vector store for relevant chunks
        # 2. Format context from top results
        # 3. Call Groq LLM with context
        # 4. Return answer + source documents
```

**What it does:**
- Takes your question
- Searches vector store for relevant information
- Sends context + question to Groq LLM
- Returns AI-generated answer with sources

## ğŸ›  System Architecture

```
ğŸ“ User Query
    â†“
ğŸ” Vector Search (find similar documents)
    â†“
ğŸ“– Context Retrieval (get top 3 chunks)
    â†“
ğŸ¤– Groq LLM (generate answer)
    â†“
âœ… Response + Sources
```

**Detailed Flow:**

1. **ğŸ“¥ Data Ingestion**: JSON publications loaded and filtered
2. **ğŸ§© Chunking**: Split into 512-token chunks with 50-token overlap
3. **ğŸ”¢ Embedding**: Each chunk converted to vector using `all-MiniLM-L6-v2`
4. **ğŸ’¾ Storage**: Vectors stored in persistent ChromaDB
5. **ğŸ” Query**: User question embedded and matched against vectors
6. **ğŸ“– Retrieval**: Top 3 relevant chunks retrieved by similarity
7. **ğŸ¤– Generation**: Groq LLM generates answer using retrieved context

## ğŸ’¡ Usage Examples

### Basic Query
```python
from rag_pipeline import GroqRAGSystem

# Initialize the system
rag = GroqRAGSystem()

# Load your data
from json_processor import load_publications_from_json
docs = load_publications_from_json("documents/project_1_publications.json")
rag.vector_store.add_documents(docs)

# Ask questions
response, sources = rag.query("What are the main topics in the publications?")
print(response)
```

### Interactive Mode
```python
# Run the main script
python rag_pipeline.py

# Then ask questions interactively:
# >>> What is discussed about AI?
# >>> Summarize the key findings
# >>> (type 'quit' to exit)
```

## ğŸ§ª Testing Your Setup

### Test 1: LLM Connection
```bash
python test_llm.py
```
âœ… **Pass**: You see a response from the LLM

### Test 2: Vector Store
```bash
python vector_store.py
```
âœ… **Pass**: See "âœ… Vector store ready!" message

### Test 3: Full RAG System
```bash
python rag_pipeline.py
```
Try: "What are the main topics?"
âœ… **Pass**: Get a relevant answer with sources

## ğŸ“¦ Dependencies

```txt
groq                    # Groq LLM API
chromadb               # Vector database
sentence-transformers  # Embedding models
tiktoken              # Token counting
python-dotenv         # Environment variables
```

## ğŸ”‘ Why These Technologies?

### ğŸš€ Groq
- âš¡ **Fast**: Up to 18x faster than standard inference
- ğŸ†“ **Free**: Generous free tier for development
- ğŸ¯ **Quality**: Access to Llama, Mixtral models
- ğŸ”§ **Simple**: OpenAI-compatible API

### ğŸ’¾ ChromaDB
- ğŸ  **Local**: Runs on your machine (no cloud needed)
- ğŸ’ª **Persistent**: Data saved between runs
- âš¡ **Fast**: Optimized similarity search
- ğŸ **Pythonic**: Easy to use API

### ğŸ¯ Sentence Transformers
- ğŸ“ **Proven**: State-of-the-art embeddings
- ğŸ“¦ **Lightweight**: `all-MiniLM-L6-v2` is only 80MB
- âš¡ **Fast**: Quick embedding generation
- ğŸ”“ **Open**: Free and open source

## âš™ï¸ Configuration

### Chunking Parameters
Edit `vector_store.py`:
```python
chunk_size = 512    # Tokens per chunk
overlap = 50        # Token overlap between chunks
```

### Search Results
Edit `rag_pipeline.py`:
```python
n_results = 3       # Number of documents to retrieve
```

### LLM Model
Edit `rag_pipeline.py`:
```python
self.model = "llama-3.1-8b-instant"  # Fast and free
# Or try: "mixtral-8x7b-32768" for longer context
```

## ğŸ› Troubleshooting

### âŒ "ModuleNotFoundError: No module named 'groq'"
**Solution:**
```bash
# Make sure virtual environment is activated
rag_env\Scripts\activate     # Windows
# source rag_env/bin/activate  # Mac/Linux

# Install packages
pip install groq chromadb sentence-transformers tiktoken python-dotenv
```

### âŒ "GROQ_API_KEY not found"
**Solution:**
1. Check `.env` file exists in project root
2. Verify format: `GROQ_API_KEY=gsk_your_key_here` (no spaces, no quotes)
3. Restart your script after creating `.env`

### âŒ "HTTPError: 401 Unauthorized"
**Solution:**
- Your Groq API key is invalid or expired
- Get a new key: https://console.groq.com/keys
- Update `.env` with new key

### âŒ ChromaDB "Database is locked"
**Solution:**
Delete the `chroma_db/` folder and rebuild:
```bash
rmdir /s chroma_db  # Windows
# rm -rf chroma_db  # Mac/Linux
python vector_store.py
```

### âŒ "No documents loaded"
**Solution:**
1. Check `documents/` folder exists
2. Verify JSON file is named correctly
3. Ensure JSON structure matches format:
```json
[{"id": "1", "title": "...", "username": "...", "publication_description": "..."}]
```

### âŒ Slow embeddings/search
**Solution:**
- First run is slower (downloads model ~80MB)
- Subsequent runs are fast
- Check your internet connection for first-time setup

## ğŸ¨ Customization

### Change Embedding Model
Edit `vector_store.py`:
```python
self.embedding_model = SentenceTransformer('all-mpnet-base-v2')  # Better quality
# Or: 'paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support
```

### Change LLM Model
Edit `rag_pipeline.py`:
```python
self.model = "mixtral-8x7b-32768"  # Longer context window
# Or: "llama-3.1-70b-versatile"  # More powerful
```

### Adjust Chunk Size
Edit `vector_store.py`:
```python
chunk_size = 1024   # Larger chunks (more context)
overlap = 100       # More overlap (better continuity)
```

### Change Number of Results
Edit `rag_pipeline.py`:
```python
results = self.vector_store.search(query, n_results=5)  # Get top 5
```

## ğŸ“Š Performance Metrics

| Component | Speed | Notes |
|-----------|-------|-------|
| Document Loading | < 1s | For ~50 publications |
| Embedding Generation | ~2-5s | First time (downloads model) |
| Vector Search | ~50ms | Per query |
| LLM Response | ~100-500ms | Depends on answer length |
| **Total Query Time** | **< 1 second** | After initial setup |

## ğŸš€ Next Improvements

Here are some ways to enhance this project:

### 1. ğŸŒ **Add Web Interface**
```bash
pip install streamlit
```
Create `app.py` with Streamlit UI for easy interaction

### 2. ğŸ”„ **Hybrid Search**
Combine vector search with keyword search (BM25) for better results

### 3. ğŸ“Š **Add Conversation Memory**
Store chat history to enable follow-up questions

### 4. ğŸ¯ **Multi-document Support**
Extend to handle PDFs, web pages, or other document types

### 5. âš¡ **Caching**
Cache frequent queries to improve response time

### 6. ğŸ“ˆ **Analytics Dashboard**
Track most asked questions, response quality, etc.

### 7. ğŸ” **User Authentication**
Add user management for multi-user scenarios

### 8. ğŸŒ **Multi-language Support**
Use multilingual embedding models

### 9. ğŸ› **Advanced Filters**
Filter by author, date, topic, etc.

### 10. ğŸ“± **API Endpoint**
Create REST API with FastAPI for integration

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

MIT License - feel free to use this project for learning and development.

## ğŸ™ Acknowledgments

- [Groq](https://groq.com/) for ultra-fast LLM inference
- [ChromaDB](https://www.trychroma.com/) for vector storage
- [Sentence Transformers](https://www.sbert.net/) for embeddings

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

**Happy Building! ğŸ‰**
